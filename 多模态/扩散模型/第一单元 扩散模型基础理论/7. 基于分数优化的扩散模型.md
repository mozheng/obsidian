这部分主要源于NCSN （Noise Conditional Score Networks）论文[2]，这是宋飏发表在 NeurIPS2019 上面的文章，比DDPM那篇文章还要早。宋飏博士认为现有的生成模型可以大体分为两种[3]：**基于似然的模型**与**隐式生成模型**。基于似然的模型要么为似然的计算对模型结构做很强的限制，要么依赖目标函数来做“近似极大似然”的训练。而隐式生成模型要求对抗训练，模型不稳定很容易崩溃。为了规避这些问题，宋飏博士选择从大量被噪声扰动的数据分布中学习分数函数（score function），即“**斯坦因分数**（Stein score）”。并使用朗之万动力学（Langevin dynamics）的方法从估计的数据分布中进行采样来生成新的样本。这样得到的生成模型通常称为“**基于分数的生成模型**”（Score-based Generative Models，SGM）。本部分不是要解析 NCSN 论文，而是要借宋飏博士的观点介绍“分数生成”。
需要注意的是，本部分有两种推理方法：基于分数匹配的郎之万动态公式 Score-Matching Langevin Dynamics (SMLD)，Tweedie 公式。我讲从这两部分介绍整体的推理过程。
### 7.1 “分数”的起源
传统生成模型的目标就是要得到数据的分布。例如一个数据集 ${x_1, x_2, ..., x_N}$ 的数据的概率密度分布（注意，这里是概率密度分布，PDF）为 $p(x)$ 。起初我们认为初始的数据是杂乱的，随机的，我们可以记为：
$$
p_{\theta}({x}) = \frac{e^{-f_{\theta}({x})}}{C_{\theta}},f_{\theta}({x})\in \mathbb{R} \tag{2.1}\\
$$
$\theta$ 是参数用于建模， $f_{\theta}$ 被称为核心能量模型（energy-based model）。这个函数型就很像高斯函数 $f(x)=\frac{1}{σ \sqrt{2π}} \cdot e^{\frac{-(x - μ)^2} {2σ^2}}$ 。我们首先计算基于 $x$ 的导数为：
$$
\nabla _{{x}}\log(p_{\theta}({x})) = -\nabla _{{x}}f _{\theta}({x}) - \nabla _{{x}}\log C_{\theta} = -\nabla _{{x}}f _{\theta}({x}) \tag{2.2} \\
$$
这里 $\nabla _{{x}}\log C_{\theta}=0$ 。此时我们突然发现，如果求解 $\nabla _{{x}}\log p_\theta({x})$ 就不需求解对 $x$ 的常数$C_\theta$了。
公式2.2的 $-\nabla _{{x}}f _{\theta}({x})$ 在这里称为“斯坦因分数”（Stein score）再相关论文中，简称为“分数”（score）。这里要明确一下，我们是对 $x$ 求导而不是类似于“极大似然计算参数”的对 $\theta$ 求导。那是因为我们在更这里关注的是数据 $x$ 采样。其原因要从“分数”具体意义解释。从数学的角度出发来看，“分数”是一个“矢(向)量场”(vector field) 。向量的方向是：对于输入数据(样本)来说，其对数概率密度增长最快的方向。如果在采样过程中沿着分数的方向走，就能够走到数据分布的高概率密度区域，最终生成的样本就会符合原数据分布。由此可知，之所以我们更关注数据 $x$ 采样，是因为，我们可以通过采样，让样本分布更趋近于“矢(向)量场”分布。
现在我们想要训练一个神经网络来估计出“分数”的分布。假设用 $s_\theta({x})$ 训练神经网络的分数，同理 $\theta$ 是代表神经网络。我们可以距离L2损失函数来最小化真实的score function，如下：
$$\mathcal{Loss} = \mathbb{E}_{p({x})}[||\nabla _{{x}}\log p({x}) - {s} _{\theta}({x})||^{2}] \tag{2.3}$$
这个损失可以叫**显式分数匹配损失**（ESM），加了”显式“二字就是为了与后面更常用的“分数匹配损失”区分，这里不做过多阐述。其中 $p(x)$ 没有下标 $\theta$ ，代指可以通过样本采样计算。
宋飏博士在噪声条件分数网络 NCSN 论文[3]中给出了另一种分数匹配损失解决方案，但本质与其相同。
### 7.2  基于分数优化的扩散模型（SMLD）
#### 7.2.1  郎之万动力学采样方法
朗之万动力学（Langevin dynamics）原是描述物理学中布朗运动（悬浮在液体或气体中的微小颗粒所做的无规则运动）的微分方程，借鉴到这里作为一种生成样本的方法。从一个分布中采样时，经常使用这种方法。概括地来说，该方法首先从先验分布随机采样一个初始样本，然后利用模型估计出来的分数逐渐将样本向数据分布的高概率密度区域靠近。为保证生成结果的多样性，我们需要采样过程带有随机性。当经过中所述的分数匹配方法训练深度生成模型后，可以使用具有迭代过程的朗之万动力学采样方法从分布中来生成新的样本。
朗之万动力学推理过程很复杂，且属于物理的知识，这里不再过多描述。采样过程描述：假设初始数据满足先验分布 $x_0 \sim \pi(x)$ ，然后使用迭代过程
$$
\begin{align*}
x_{t+1}&=x_t+\frac{\epsilon}{2}\nabla _{x}\log  p\left( x \right) +\sqrt{\epsilon}\boldsymbol{z}_t, t=0,1,2,\cdots ,T\\
&=x_t+{\tau}\nabla _{x}\log  p\left( x \right) +\sqrt{2\tau}\boldsymbol{z}_t, t=0,1,2,\cdots ,T \tag{2.4}\\
\end{align*}
$$
当 $\frac{\epsilon}{2}=\tau$ 时上面的公式等价。其中， $z_i \sim \mathcal{N}(0,I)$ 为标准高斯分布，可以看成是噪声的概念，$\epsilon,\tau$ 是可以看成步长。虽然物理定义不同，但“朗之万动力学公式”本质与“随机梯度下降”，“泰勒公式展开”实际上是同源的。例如我们在高等数学中学到的泰勒展开公式如下：
$$
\begin{align}
f(x+\epsilon)=f(x)+f^{'}(x)\epsilon+\frac{1}{2}f^{''}(x)\epsilon^2+o(\epsilon^3)
\end{align}
$$
再看“梯度下降”的公式：
$$
\begin{align}
f(k-\epsilon) =f(k)  - \epsilon * f'(k) \\
x_{k+1} = x_k - \epsilon*f'(k)
\end{align}
$$
为了适配“梯度下降”负梯度求最小值的逻辑，这里的 $\epsilon$ 是个负值。但是这并不否认“泰勒公式展开”同源的概念。为什么总是“泰勒公式展开”在起关键作用。其本质在于泰勒公式是在一步步靠近极值，且越靠近极值，梯度越等于0越不更新。所以以后我们在极值附近考虑问题时，可以优先考虑“泰勒公式展开”方案。因为有 $\sqrt{2\tau}z_t$ 项的作用，“朗之万动力学公式”不是紧贴极值采样，而是在极值附近采样；而且由于 $z_t$ 的随机作用，采样会随机的聚合在极值附近。采样的目的自然也是为了更好的描述整体分布。理论上，在公式（2.4）中，当 $t \rightarrow \infty$ 时，$\epsilon \rightarrow 0$ ，最终生成的样本 $x_t$ 将会服从原数据分布 $q_{data}(x)$，趋于某一个特定值。（如图2.1）
我们可以看到该迭代采样过程的两个性质：1. 在这个序列中，相邻两个变量存在关系，不相邻的两个变量无关。2. 整体结果趋近于稳态。这种性质自然就和马尔可夫链产生关系。

![](images/2.2.2.jpg)
图 2.1 从左到右，图片模拟的是使用“郎之万动力学”在两个高斯分布中采样，最后采样的结果（右图）很符合原始两个峰值的高斯分布

#### 7.2.2 基于分数优化的生成模式
思路先从“郎之万动力学公式”回来，我们的目的是还是要做图像生成。当前的问题是：不知道**原始数据分布，更不知道的数据梯度向量**。有一种解题方法是从标准高斯噪声中构建新分布。我们考虑从标准高斯分布 $\mathcal{N}(0,𝐼)$ 中采样随机噪声𝜖，然后乘上我们预定义的 $𝜎$ ，接着加到样本 $𝑥$ 中，从而就能得到加噪后的样本 $\tilde{𝑥}$。这种思路与VAE的重参数化技巧一致。我们构建新分布为：
$$\begin{align}
q_{\sigma}(\tilde{x}) &=\int q_σ(\tilde{x}|x) q_{data}(x) dx \tag{2.5} \\
& = \int \mathcal{N}(\tilde{x}|x,σ^2I)q_{data}(x) dx \tag{2.6}\\
\text{and} ~ \tilde{x}&=x+\sigma\epsilon, ~ \epsilon\sim \mathcal{N}(0,I) \tag{2.7} \\
\end{align}$$公式（2.5）的积分是代表采样，有些地方会看到公式（2.6）的形式，因为已经重参数化成正态分布。**从公式描述上我们也知道 $q_{data}$ 函数其实就是 $q_σ$ ，只不过是通过分布采样得到函数。** $q_{data}(x)$ 代表需要扰动的数据的采样分布。$q_σ(\tilde{x}|x) \sim \mathcal{N}(\tilde{x}|x,σ^2I)$ 表示扰动噪声数据分布。$q_{\sigma}(\tilde{x})$ 就是我们构建的新分布。$\tilde{𝑥}$就是我们加噪的新样本。噪声条件分数网络是分数模型的一个使用特例。
***
说明：宋飏博士在论文与blog有不同的标记方法。论文更准确用的是 $q_{\sigma}(x)$ ，代表与 $p(x)$ 不同；blog上用的是 $p_{\sigma}(x)$ ，为了在后面从公式上直接替换 $q_{data}(x)$ 。两者都没有错，就是解释不同。
***
NCSN的目标是训练一个条件分数网络 $s_{\theta}( \tilde{x},\sigma )$ 来估计扰动数据的分布，即 
$$
\frac{1}{2}\mathbb{E}_{q_σ(\tilde{x})}
\Big[\Vert s_{\theta}( \tilde{x},\sigma ) - \nabla _{\tilde{x}} \log q_σ(\tilde{x})\Vert_2^2\Big] \tag{2.8}
$$
我们依据DSM论文[4]，得到以下的推导公式
$$
\begin{align}
& ~~~~ \frac{1}{2}\mathbb{E}_{q_σ(\tilde{x})}
\Big[\Vert s_{\theta}( \tilde{x},\sigma ) - \nabla _{\tilde{x}} \log q_σ(\tilde{x})\Vert_2^2\Big] \tag{2.9} \\
&=\frac{1}{2}\int {q_σ(\tilde{x})}
\Big[\Vert s_{\theta}( \tilde{x},\sigma ) - \frac{1}{q_σ(\tilde{x})}\frac{\partial q_σ(\tilde{x})}{\partial \tilde{x}} \Vert_2^2\Big] d\tilde{x}\\
&=\frac{1}{2}\int 
\Big[\Vert {q_σ(\tilde{x})}s_{\theta}( \tilde{x},\sigma ) - \frac{\partial q_σ(\tilde{x})}{\partial \tilde{x}} \Vert_2^2\Big] d\tilde{x}\\
&=\frac{1}{2}\int 
\Big[\Vert {q_σ(\tilde{x})}s_{\theta}( \tilde{x},\sigma ) - \frac{\partial \int q_σ(\tilde{x}|x) q_σ(x) dx}{\partial \tilde{x}} \Vert_2^2\Big] d\tilde{x} \\
&=\frac{1}{2}\int 
\Big[\Vert {q_σ(\tilde{x})}s_{\theta}( \tilde{x},\sigma ) - \int \frac{\partial q_σ(\tilde{x}|x)}{\partial \tilde{x}} q_σ(x) dx \Vert_2^2\Big] d\tilde{x} \\
&=\frac{1}{2}\int 
\Big[\Vert {\int q_σ(\tilde{x}|x) q_σ(x) dx} \cdot s_{\theta}( \tilde{x},\sigma ) - \int q_σ(\tilde{x}|x) q_σ(x)\frac{\partial \log q_σ(\tilde{x}|x)}{\partial \tilde{x}} dx \Vert_2^2\Big] d\tilde{x} \\
&=\frac{1}{2}\int_{\tilde{x}} \int_x q_σ(\tilde{x}|x) q_σ(x) 
\Big[\Vert s_{\theta}( \tilde{x},\sigma ) - \frac{\partial \log q_σ(\tilde{x}|x)}{\partial \tilde{x}}\Vert_2^2\Big] dx d\tilde{x} \\
&= \frac{1}{2}\mathbb{E}_{q_σ(\tilde{x}|x) q_σ(x) }
\Big[\Vert s_{\theta}( \tilde{x},\sigma ) - \nabla _{\tilde{x}} \log q_σ(\tilde{x}|x)\Vert_2^2\Big] \tag{2.10}
\end{align}
$$
公式（2.10）就是去噪声分数匹配的损失函数。去噪分数匹配（Denoising Score Matching，DSM）方法是在计算噪声条件分数网络（NCSN） 中默认的计算方法[3]。
这里面还有一个潜在推理。当 $x$ 是 $D$ 维（如果 $f(x)$ 是一维高斯，下面结果会更容易得出了，这里不做解释。），即 ${x}\in \mathbb{R} ^D$ 时，由多维高斯分布 $f( \mathbf{x} ) = \frac{1}{(2\pi)^{\frac{D}{2}} | Σ |^{\frac{1}{2}}} e^{- \frac{1}{2}( x - μ )^T Σ^{-1}(x - μ)}$  可得:
$$
\begin{align}
\nabla _{\tilde{x}} \log q_σ(\tilde{x}|x) &= \nabla _{\tilde{x}}(C - \frac{1}{2}(\tilde{x} - x )^T Σ^{-1}(\tilde{x} - x)) \\
&= -Σ^{-1}(\tilde{x} - x) \\
&= -\begin{bmatrix} 𝜎_{1}^2 & & & \\ & 𝜎_{2}^2 & & \\ & & \ddots &\\ & & &𝜎_{d}^2 \end{bmatrix}^{-1} (\tilde{x} - x) \tag{2.11}\\
&= - \frac{\tilde{x} - x}{𝜎^2} \tag{2.12} \\
&= - \frac{\epsilon}{𝜎} \tag{2.13}
\end{align}
$$
 ，将（2.12）带入公式（2.10）可以写为如下形式：
$$
\begin{align}
& ~~~~ \frac{1}{2}\mathbb{E}_{q_σ(\tilde{x}|x) q_{data}(x) }
\Big[\Vert s_{\theta}( \tilde{x},\sigma ) - \nabla _{\tilde{x}} \log q_σ(\tilde{x}|x)\Vert_2^2\Big] \\
&= \frac{1}{2}\mathbb{E}_{q_σ(\tilde{x}|x)}\mathbb{E}_{q_{data}(x)}
\Big[\Big\Vert s_{\theta}( \tilde{x},\sigma ) + \frac{\tilde{x}-x}{σ^2}\Big\Vert_2^2\Big] \\ 
&= \frac{1}{2}\mathbb{E}_{q_{data}(x)}\mathbb{E}_{\tilde{x} \sim \mathcal{N}(\tilde{x}|x,σ^2I)}
\Big[\Big\Vert s_{\theta}( \tilde{x},\sigma ) + \frac{\tilde{x}-x}{σ^2}\Big\Vert_2^2\Big] \tag{2.14} \\
&= \frac{1}{2}\mathbb{E}_{q_{data}(x)}\mathbb{E}_{\tilde{x} \sim \mathcal{N}(\tilde{x}|x,σ^2I)}
\Big[\Big\Vert s_{\theta}( \tilde{x},\sigma ) + \frac{\epsilon}{σ}\Big\Vert_2^2\Big]  \\
& =\mathcal{l}(\theta;\sigma) \tag{2.15}
\end{align}
$$
这种优化方法可以套回VDM优化模式，但是不太直观。我们下面介绍一个更直观的推理，基于Tweedie 公式的分数推理。

***
尝试思考：在前文讲VDM时，是方差位未知，这里还是方差位参数未知。他们是不是有什么联系呢？
***
### 7.3 基于分数优化的扩散模型（Tweedie 公式）
介绍完“分数”的基础知识，我们回到扩散模型的理论。为了得出扩散模型的分数函数的优化策略，我们引入 Tweedie 公式。：Tweedie 公式来自贝叶斯估计。贝叶斯估计的问题定义为根据一些观测数据 $𝑥$ 来估计未知参数 $𝜃$，如果用均方误差(MSE)损失函数来衡量估计的准确性的话，我们将问题建模为：
$$
L= \mathbb{E}[(\hat{\theta}(x)-\theta)^2]
$$
整个问题本质其实就是求解， $x$ 的条件下， $\theta$ 值的期望：
$$
\hat{\theta}(x)=\mathbb{E}[\theta|x]=\int \theta p(\theta|x)d\theta \tag{1.120}
$$
而Tweedie公式，就是一种估计 $\theta$ 的方案。假设 $p(x|\theta)=\mathcal{N}(\theta,\sigma^2)$，可以通过观测数据估计出参数𝜎，则有：
$$
\begin{align}
\hat{\theta}(x)=\mathbb{E}[\theta|x] &=\int \theta p(\theta|x)d\theta \\
&= x+\sigma^2\frac{d}{dx} \log p(x) \tag{1.121}
\end{align}
$$
这个公式的优点是一直保有着 $𝑝(𝜃)$ 的雏形而没有探究它的具体样子。此公式专供已知方差，求不出来均值时使用。从数学上讲，对于满足高斯分布的变量 $𝑧 \sim \mathcal{𝑁}(𝑧;μ_𝑧,Σ_𝑧)$，Tweedie 公式如下：
$$
\mathbb{E}[𝜇_z|z]=z+𝛴_z ∇_z \log⁡ p(z) \tag{1.122}
$$
此时我们构造出了”分数“公式。我们从前文的推理 $q(x_t|x_0)=\mathcal{N}(x_t;\sqrt{\bar{\alpha}_t}x_0,(1-\bar{\alpha}_t)I)$ 开始，尝试将上面的推论用到之前VDM的推理。利用Tweedie公式，我们得出：
$$
\mathbb{𝐸}[𝜇_{x_t}|x_t]=x_t+(1-\bar{\alpha}_t)∇_{x_t} \log⁡ 𝑝(x_t) \tag{1.123}
$$
这么做的目的是我们发现公式 $q(x_t|x_0)=\mathcal{N}(x_t;\sqrt{\bar{\alpha}_t}x_0,(1-\bar{\alpha}_t)I)$  里面“方差很好求，均值太难求（因为有$x_0$）“。我们有必要避免较少对均值的的使用，如有必要还需要对均值给出一种新的形式。为了符号的简洁性，我们将 $∇_{x_t}\log p(x_t)$ 写为 $∇ \log p(x_t)$ 。根据 Tweedie 公式，由 $𝑥_𝑡$ 生成的真实均值 $μ_{𝑥_𝑡}=\sqrt{\bar{α}_t}𝑥_0$ ，可定义为：
$$
\begin{align}
\sqrt{\bar{α}_t}𝑥_0 &= x_t+(1-\bar{\alpha}_t)∇_{x_t} \log⁡ 𝑝(x_t) \tag{1.124} \\
\therefore x_0 &= \frac{x_t+(1-\bar{\alpha}_t)∇_{x_t} \log⁡ 𝑝(x_t)} {\sqrt{\bar{α}_t}} \tag{1.125}
\end{align}
$$
然后，我们可以将公式（1.125）再次代入我们的真实去噪转移均值 $μ_𝑞(𝑥_𝑡,𝑥_0)$并推导出新的形式：
$$
\begin{align}
\mu_q(x_t,x_0) &=\frac{\sqrt{{\alpha}_t}(1-\bar{{\alpha}}_{t-1}){{x}}_t+\sqrt{\bar{{\alpha}}_{t-1}}(1-{\alpha}_t){{x}}_{0}}{1-\bar{{\alpha}}_{t}} 
\tag{1.126} \\
&= \frac{\sqrt{{\alpha}_t}(1-\bar{{\alpha}}_{t-1}){{x}}_t+\sqrt{\bar{{\alpha}}_{t-1}}(1-{\alpha}_t)\frac{x_t+(1-\bar{\alpha}_t)∇ \log⁡ 𝑝(x_t)} {\sqrt{\bar{α}_t}} }{1-\bar{{\alpha}}_{t}} \tag{1.127} \\
&= \frac{\sqrt{{\alpha}_t}(1-\bar{{\alpha}}_{t-1}){{x}}_t+(1-{\alpha}_t)\frac{x_t+(1-\bar{\alpha}_t)∇ \log⁡ 𝑝(x_t)} {\sqrt{α_t}} }{1-\bar{\alpha}_{t}} \tag{1.128} \\
&= \frac{\sqrt{{\alpha}_t}(1-\bar{{\alpha}}_{t-1}){{x}}_t}{1-\bar{\alpha}_{t}} +
\frac{(1-{\alpha}_t)x_t}{(1-\bar{\alpha}_{t})\sqrt{α_t}} + \frac{(1-\alpha_t)(1-\bar{\alpha}_t)∇ \log⁡ 𝑝(x_t) }{(1-\bar{\alpha}_{t})\sqrt{α_t}} \tag{1.129} \\
&= \Big(\frac{\sqrt{{\alpha}_t}(1-\bar{{\alpha}}_{t-1})}{1-\bar{\alpha}_{t}} +
\frac{1-{\alpha}_t}{(1-\bar{\alpha}_{t})\sqrt{α_t}} \Big)x_t+ \frac{1-\alpha_t}{\sqrt{α_t}}∇ \log⁡ 𝑝(x_t) \tag{1.130} \\
&= \Big(\frac{{\alpha}_t(1-\bar{{\alpha}}_{t-1})}{(1-\bar{\alpha}_{t})\sqrt{{\alpha}_t}} +
\frac{1-{\alpha}_t}{(1-\bar{\alpha}_{t})\sqrt{α_t}} \Big)x_t+ \frac{1-\alpha_t}{\sqrt{α_t}}∇ \log⁡ 𝑝(x_t) \tag{1.131} \\
&= \frac{{\alpha}_t -\bar{{\alpha}}_{t}+ 1-{\alpha}_t}{(1-\bar{\alpha}_{t})\sqrt{α_t}}x_t+ \frac{1-\alpha_t}{\sqrt{α_t}}∇ \log⁡ 𝑝(x_t) \tag{1.132} \\
&= \frac{ 1-\bar{{\alpha}}_{t}}{(1-\bar{\alpha}_{t})\sqrt{α_t}}x_t+ \frac{1-\alpha_t}{\sqrt{α_t}}∇ \log⁡ 𝑝(x_t) \tag{1.133} \\
&= \frac{ 1}{\sqrt{α_t}}x_t+ \frac{1-\alpha_t}{\sqrt{α_t}}∇ \log⁡ 𝑝(x_t) \tag{1.134} \\
\end{align}
$$
因此，我们可以设置近视去噪均值 $\mu_\theta(x_t,t)$ 为：
$$
\mu_\theta(x_t,t) = \frac{1}{\sqrt{\alpha_t}}x_t - \frac{1-{\alpha}_t}{\sqrt{1-\bar{{\alpha}}_t}\sqrt{\alpha_t}}s_\theta(x_t,t) \tag{1.135}
$$
相应的优化问题可以变为：
$$
\begin{align}
&~~~~\arg\min_{{{\theta}}} D_{\text{KL}}(q({{x}}_{t-1}|{{x}}_t,{{x}}_0)\Vert p_{{\theta}}({{x}}_{t-1}|{{x}}_t))\\
&=\arg\min_{{{\theta}}} D_{\text{KL}}(\mathcal{N} ({{x}}_{t-1}; μ_q, Σ_q(t)) \Vert \mathcal{N} ({{x}}_{t-1}; μ_{{\theta}}, Σ_q(t))) \tag{1.136} \\
&=\arg\min_{{{\theta}}} \frac{1}{2σ_q^2 (t)} \left[ \Big\Vert \frac{1}{\sqrt{\alpha_t}}x_t + \frac{1-{\alpha}_t}{\sqrt{\alpha_t}}s_\theta(x_t,t) − \frac{1}{\sqrt{\alpha_t}}x_t - \frac{1-{\alpha}_t}{\sqrt{\alpha_t}}∇ \log⁡ 𝑝(x_t)\Big\Vert_2^2 \right] \tag{1.137} \\
&=\arg\min_{{{\theta}}} \frac{1}{2σ_q^2 (t)} \left[ \Big\Vert  \frac{1-{\alpha}_t}{\sqrt{\alpha_t}}s_\theta(x_t,t) - \frac{1-{\alpha}_t}{\sqrt{\alpha_t}}∇ \log⁡ 𝑝(x_t)\Big\Vert_2^2 \right] \tag{1.138} \\
&=\arg\min_{{{\theta}}} \frac{1}{2σ_q^2 (t)} \left[ \Big\Vert  \frac{1-{\alpha}_t}{\sqrt{\alpha_t}}(s_\theta(x_t,t) - ∇ \log⁡ 𝑝(x_t))\Big\Vert_2^2 \right] \tag{1.139} \\
&=\arg\min_{{{\theta}}} \frac{1}{2σ_q^2 (t)} \frac{(1-{\alpha}_t)^2}{\alpha_t} \left[\Vert s_\theta(x_t,t) - ∇ \log⁡ 𝑝(x_t)\Vert_2^2 \right] \tag{1.140} \\
\end{align}
$$
这里，$s_θ(x_t, t)$ 可以是一个神经网络，用来学习预测分数函数（score function） $∇_{x_t} \log  p(x_t)$。 $∇_{x_t} \log  p(x_t)$ 就是分数函数， 其性质后面会详细阐述。敏锐的读者会注意到，分数函数 $∇ \log⁡ p(x_t)$ 在形式上与源噪声 $𝜖_0$ 非常相似。将Tweedie公式（1.125）与重参数化技巧公式（1.105）结合起来，可以明确地展示这一点：
$$
\begin{align}
x_0 = \frac{x_t+(1-\bar{\alpha}_t)∇ \log⁡ 𝑝(x_t)} {\sqrt{\bar{α}_t}} &= \frac{x_t-\sqrt{1-\bar{\alpha}_t}\epsilon_0}{\sqrt{\bar{\alpha}_t}} \tag{1.141} \\
\therefore (1-\bar{\alpha}_t)∇ \log⁡ 𝑝(x_t) &= -\sqrt{1-\bar{\alpha}_t}\epsilon_0 \tag{1.142} \\
∇ \log⁡ 𝑝(x_t) &= - \frac{1}{\sqrt{1-\bar{\alpha}_t}}\epsilon_0 \tag{1.143} \\
\end{align}
$$

不管是从郎之万公式得到的
$$
\begin{align}
\nabla _{\tilde{x}} \log q_σ(\tilde{x}|x) &= - \frac{\epsilon}{𝜎} \\
q(x_t|x_0)&=\mathcal{N}(x_t;\sqrt{\bar{\alpha}_t}x_0,(1-\bar{\alpha}_t)I)
\end{align}
$$
还是Tweedie公式得到的 $∇ \log⁡ 𝑝(x_t) = - \frac{1}{\sqrt{1-\bar{\alpha}_t}}\epsilon_0$ 我们都能得到
$$
\arg\min_{{{\theta}}} D_{\text{KL}}(q({{x}}_{t-1}|{{x}}_t,{{x}}_0)\Vert p_{{\theta}}({{x}}_{t-1}|{{x}}_t)) = \arg\min_{{{\theta}}} \frac{1}{2σ_q^2 (t)} \frac{(1-{\alpha}_t)^2}{\alpha_t} \left[\Vert s_\theta(x_t,t) + \frac{1}{\sqrt{1-\bar{\alpha}_t}}\epsilon_0 \Vert_2^2 \right]
$$
从目前来讲，我们用数学证明证实了：学习神经网络预测原始图像 $𝑥_0$；学习神经网络预测源噪声 $𝜖_0$；学习一定噪声水平下的图像分数函数 $∇ \log⁡ p(x_t)$。都可以优化VDM函数。我们在看论文时如果遇到某些人用一种方法推理，请不要惊慌。其实内部原理是一样的。



[1] Jonathan Ho, Ajay Jain, Pieter Abbeel. Denoising Diffusion Probabilistic Models. arXiv preprint  arXiv:2006.11239v2.
[2] Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, Ben Poole. Score-Based Generative Modeling through Stochastic Differential Equations. arXiv:2011.13456
[3] Yang Song, and Stefano Ermon. Generative Modeling by Estimating Gradients of the Data Distribution. arXiv:1907.05600
[4] Pascal Vincent. A Connection Between Score Matching and Denoising Autoencoders