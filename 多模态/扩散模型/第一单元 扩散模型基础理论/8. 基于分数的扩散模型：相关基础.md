### 8.1 分数生成模型
这部分主要源于NCSN （Noise Conditional Score Networks）论文[2]，这是宋飏发表在 NeurIPS2019 上面的文章，比DDPM那篇文章还要早。宋飏博士认为现有的生成模型可以大体分为两种[3]：**基于似然的模型**与**隐式生成模型**。基于似然的模型要么为似然的计算对模型结构做很强的限制，要么依赖目标函数来做“近似极大似然”的训练。而隐式生成模型要求对抗训练，模型不稳定很容易崩溃。为了规避这些问题，宋飏博士选择从大量被噪声扰动的数据分布中学习分数函数（score function），即“**斯坦因分数**（Stein score）”。并使用朗之万动力学（Langevin dynamics）的方法从估计的数据分布中进行采样来生成新的样本。这样得到的生成模型通常称为“**基于分数的生成模型**”（Score-based Generative Models，SGM）。
### 8.1.1 “分数”的起源
传统生成模型的目标就是要得到数据的分布。例如一个数据集 ${x_1, x_2, ..., x_N}$ 的数据的概率密度分布（注意，这里是概率密度分布，PDF）为 $p(x)$ 。起初我们认为初始的数据是杂乱的，随机的，我们可以记为：
$$
p_{\theta}({x}) = \frac{e^{-f_{\theta}({x})}}{C_{\theta}},f_{\theta}({x})\in \mathbb{R} \tag{2.1}\\
$$
$\theta$ 是参数用于建模， $f_{\theta}$ 被称为核心能量模型（energy-based model）。这个函数型就很像高斯函数 $f(x)=\frac{1}{σ \sqrt{2π}} \cdot e^{\frac{-(x - μ)^2} {2σ^2}}$ 。我们首先计算基于 $x$ 的导数为：
$$
\nabla _{{x}}\log(p_{\theta}({x})) = -\nabla _{{x}}f _{\theta}({x}) - \nabla _{{x}}\log C_{\theta} = -\nabla _{{x}}f _{\theta}({x}) \tag{2.2} \\
$$
这里 $\nabla _{{x}}\log C_{\theta}=0$ 。此时我们突然发现，如果求解 $\nabla _{{x}}\log p_\theta({x})$ 就不需求解对 $x$ 的常数$C_\theta$了。
公式2.2的 $-\nabla _{{x}}f _{\theta}({x})$ 在这里称为“斯坦因分数”（Stein score）再相关论文中，简称为“分数”（score）。这里要明确一下，我们是对 $x$ 求导而不是类似于“极大似然计算参数”的对 $\theta$ 求导。那是因为我们在更这里关注的是数据 $x$ 采样。其原因要从“分数”具体意义解释。从数学的角度出发来看，“分数”是一个“矢(向)量场”(vector field) 。向量的方向是：对于输入数据(样本)来说，其对数概率密度增长最快的方向。如果在采样过程中沿着分数的方向走，就能够走到数据分布的高概率密度区域，最终生成的样本就会符合原数据分布。由此可知，之所以我们更关注数据 $x$ 采样，是因为，我们可以通过采样，让样本分布更趋近于“矢(向)量场”分布。
现在我们想要训练一个神经网络来估计出“分数”的分布。假设用 $s_\theta({x})$ 训练神经网络的分数，同理 $\theta$ 是代表神经网络。我们可以距离L2损失函数来最小化真实的score function，如下：
$$\mathcal{Loss} = \mathbb{E}_{p({x})}[||\nabla _{{x}}\log p({x}) - {s} _{\theta}({x})||^{2}] \tag{2.3}$$
这个损失可以叫**显式分数匹配损失**（ESM），加了”显式“二字就是为了与后面更常用的“分数匹配损失”区分，这里不做过多阐述。其中 $p(x)$ 没有下标 $\theta$ ，代指可以通过样本采样计算。
宋飏博士在噪声条件分数网络 NCSN 论文[3]中给出了另一种分数匹配损失解决方案，但本质与其相同。
### 8.2  郎之万动力学采样方法
朗之万动力学（Langevin dynamics）原是描述物理学中布朗运动（悬浮在液体或气体中的微小颗粒所做的无规则运动）的微分方程，借鉴到这里作为一种生成样本的方法。从一个分布中采样时，经常使用这种方法。概括地来说，该方法首先从先验分布随机采样一个初始样本，然后利用模型估计出来的分数逐渐将样本向数据分布的高概率密度区域靠近。为保证生成结果的多样性，我们需要采样过程带有随机性。当经过中所述的分数匹配方法训练深度生成模型后，可以使用具有迭代过程的朗之万动力学采样方法从分布中来生成新的样本。
朗之万动力学推理过程很复杂，且属于物理的知识，这里不再过多描述。采样过程描述：假设初始数据满足先验分布 $x_0 \sim \pi(x)$ ，然后使用迭代过程
$$
\begin{align*}
x_{t+1}&=x_t+\frac{\epsilon}{2}\nabla _{x}\log  p\left( x \right) +\sqrt{\epsilon}\boldsymbol{z}_t, t=0,1,2,\cdots ,T\\
&=x_t+{\tau}\nabla _{x}\log  p\left( x \right) +\sqrt{2\tau}\boldsymbol{z}_t, t=0,1,2,\cdots ,T \tag{2.4}\\
\end{align*}
$$
上面的公式等价，只是不同的记法。其中， $z_i \sim \mathcal{N}(0,I)$ 为标准高斯分布，可以看成是噪声的概念，$\epsilon,\tau$ 是可以看成步长。
虽然物理定义不同，但“朗之万动力学公式”本质与“随机梯度下降”，“泰勒公式展开”在格式上是同源的。例如我们在高等数学中学到的泰勒展开公式如下：
$$
\begin{align}
f(x+\epsilon)=f(x)+f^{'}(x)\epsilon+\frac{1}{2}f^{''}(x)\epsilon^2+o(\epsilon^3)
\end{align}
$$
再看“梯度下降”的公式：
$$
\begin{align}
f(k-\epsilon) =f(k)  - \epsilon * f'(k) \\
x_{k+1} = x_k - \epsilon*f'(k)
\end{align}
$$
为了适配“梯度下降”负梯度求最小值的逻辑，这里的 $\epsilon$ 是个负值。但是这并不否认“泰勒公式展开”同源的概念。为什么总是“泰勒公式展开”在起关键作用。其本质在于泰勒公式是在一步步靠近极值，且越靠近极值，梯度越等于0越不更新。所以以后我们在极值附近考虑问题时，可以优先考虑“泰勒公式展开”方案。因为有 $\sqrt{2\tau}z_t$ 项的作用，“朗之万动力学公式”不是紧贴极值采样，而是在极值附近采样；而且由于 $z_t$ 的随机作用，采样会随机的聚合在极值附近。采样的目的自然也是为了更好的描述整体分布。理论上，在公式（2.4）中，当 $t \rightarrow \infty$ 时，$\epsilon \rightarrow 0$ ，最终生成的样本 $x_t$ 将会服从原数据分布 $q_{data}(x)$，趋于某一个特定值。（如图2.1）
我们可以看到该迭代采样过程的两个性质：1. 在这个序列中，相邻两个变量存在关系，不相邻的两个变量无关。2. 整体结果趋近于稳态。这种性质自然就和马尔可夫链产生关系。

![](images/2.2.2.jpg)
图 2.1 从左到右，图片模拟的是使用“郎之万动力学”在两个高斯分布中采样，最后采样的结果（右图）很符合原始两个峰值的高斯分布

### 8.3 分数生成推理过程
### 8.3.1  噪声条件分数网络（Noise Conditional Score Networks，NCSN）
思路先从“郎之万动力学公式”回来，我们的目的是还是要做图像生成。当前的问题是：不知道**原始数据分布，更不知道的数据梯度向量**。有一种解题方法是从标准高斯噪声中构建新分布。我们考虑从标准高斯分布 $\mathcal{N}(0,𝐼)$ 中采样随机噪声𝜖，然后乘上我们预定义的 $𝜎$ ，接着加到样本 $𝑥$ 中，从而就能得到加噪后的样本 $\tilde{𝑥}$。这种思路与VAE的重参数化技巧一致。我们构建新分布为：
$$\begin{align}
q_{\sigma}(\tilde{x}) &=\int q_σ(\tilde{x}|x) q_{data}(x) dx \tag{2.5} \\
& = \int \mathcal{N}(\tilde{x}|x,σ^2I)q_{data}(x) dx \tag{2.6}\\
\text{and} ~ \tilde{x}&=x+\sigma\epsilon, ~ \epsilon\sim \mathcal{N}(0,I) \tag{2.7} \\
\end{align}$$公式（2.5）的积分是代表采样，有些地方会看到公式（2.6）的形式，因为已经重参数化成正态分布。**从公式描述上我们也知道 $q_{data}$ 函数其实就是 $q_σ$ ，只不过是通过分布采样得到函数。** $q_{data}(x)$ 代表需要扰动的数据的采样分布。$q_σ(\tilde{x}|x) \sim \mathcal{N}(\tilde{x}|x,σ^2I)$ 表示扰动噪声数据分布。$q_{\sigma}(\tilde{x})$ 就是我们构建的新分布。$\tilde{𝑥}$就是我们加噪的新样本。噪声条件分数网络是分数模型的一个使用特例。
***
说明：宋飏博士在论文与blog有不同的标记方法。论文更准确用的是 $q_{\sigma}(x)$ ，代表与 $p(x)$ 不同；blog上用的是 $p_{\sigma}(x)$ ，为了在后面从公式上直接替换 $q_{data}(x)$ 。两者都没有错，就是解释不同。
***
NCSN的目标是训练一个条件分数网络 $s_{\theta}( \tilde{x},\sigma )$ 来估计扰动数据的分布，即 
$$
\frac{1}{2}\mathbb{E}_{q_σ(\tilde{x})}
\Big[\Vert s_{\theta}( \tilde{x},\sigma ) - \nabla _{\tilde{x}} \log q_σ(\tilde{x})\Vert_2^2\Big] \tag{2.8}
$$
我们依据DSM论文[4]，得到以下的推导公式
$$
\begin{align}
& ~~~~ \frac{1}{2}\mathbb{E}_{q_σ(\tilde{x})}
\Big[\Vert s_{\theta}( \tilde{x},\sigma ) - \nabla _{\tilde{x}} \log q_σ(\tilde{x})\Vert_2^2\Big] \tag{2.9} \\
&=\frac{1}{2}\int {q_σ(\tilde{x})}
\Big[\Vert s_{\theta}( \tilde{x},\sigma ) - \frac{1}{q_σ(\tilde{x})}\frac{\partial q_σ(\tilde{x})}{\partial \tilde{x}} \Vert_2^2\Big] d\tilde{x}\\
&=\frac{1}{2}\int 
\Big[\Vert {q_σ(\tilde{x})}s_{\theta}( \tilde{x},\sigma ) - \frac{\partial q_σ(\tilde{x})}{\partial \tilde{x}} \Vert_2^2\Big] d\tilde{x}\\
&=\frac{1}{2}\int 
\Big[\Vert {q_σ(\tilde{x})}s_{\theta}( \tilde{x},\sigma ) - \frac{\partial \int q_σ(\tilde{x}|x) q_σ(x) dx}{\partial \tilde{x}} \Vert_2^2\Big] d\tilde{x} \\
&=\frac{1}{2}\int 
\Big[\Vert {q_σ(\tilde{x})}s_{\theta}( \tilde{x},\sigma ) - \int \frac{\partial q_σ(\tilde{x}|x)}{\partial \tilde{x}} q_σ(x) dx \Vert_2^2\Big] d\tilde{x} \\
&=\frac{1}{2}\int 
\Big[\Vert {\int q_σ(\tilde{x}|x) q_σ(x) dx} \cdot s_{\theta}( \tilde{x},\sigma ) - \int q_σ(\tilde{x}|x) q_σ(x)\frac{\partial \log q_σ(\tilde{x}|x)}{\partial \tilde{x}} dx \Vert_2^2\Big] d\tilde{x} \\
&=\frac{1}{2}\int_{\tilde{x}} \int_x q_σ(\tilde{x}|x) q_σ(x) 
\Big[\Vert s_{\theta}( \tilde{x},\sigma ) - \frac{\partial \log q_σ(\tilde{x}|x)}{\partial \tilde{x}}\Vert_2^2\Big] dx d\tilde{x} \\
&= \frac{1}{2}\mathbb{E}_{q_σ(\tilde{x}|x) q_σ(x) }
\Big[\Vert s_{\theta}( \tilde{x},\sigma ) - \nabla _{\tilde{x}} \log q_σ(\tilde{x}|x)\Vert_2^2\Big] \tag{2.10}
\end{align}
$$
公式（2.10）就是去噪声分数匹配的损失函数。去噪分数匹配（Denoising Score Matching，DSM）方法是在计算噪声条件分数网络（NCSN） 中默认的计算方法[3]。
这里面还有一个潜在推理。当 $x$ 是 $D$ 维（如果 $f(x)$ 是一维高斯，下面结果会更容易得出了，这里不做解释。），即 ${x}\in \mathbb{R} ^D$ 时，由多维高斯分布 $f( \mathbf{x} ) = \frac{1}{(2\pi)^{\frac{D}{2}} | Σ |^{\frac{1}{2}}} e^{- \frac{1}{2}( x - μ )^T Σ^{-1}(x - μ)}$  可得:
$$
\begin{align}
\nabla _{\tilde{x}} \log q_σ(\tilde{x}|x) &= \nabla _{\tilde{x}}(C - \frac{1}{2}(\tilde{x} - x )^T Σ^{-1}(\tilde{x} - x)) \\
&= -Σ^{-1}(\tilde{x} - x) \\
&= -\begin{bmatrix} 𝜎_{1}^2 & & & \\ & 𝜎_{2}^2 & & \\ & & \ddots &\\ & & &𝜎_{d}^2 \end{bmatrix} (\tilde{x} - x) \tag{2.11}\\
&= - \frac{\tilde{x} - x}{𝜎^2} \tag{2.12} \\
&= - \frac{\epsilon}{𝜎} \tag{2.13}
\end{align}
$$
 ，将（2.12）带入公式（2.10）可以写为如下形式：
$$
\begin{align}
& ~~~~ \frac{1}{2}\mathbb{E}_{q_σ(\tilde{x}|x) q_{data}(x) }
\Big[\Vert s_{\theta}( \tilde{x},\sigma ) - \nabla _{\tilde{x}} \log q_σ(\tilde{x}|x)\Vert_2^2\Big] \\
&= \frac{1}{2}\mathbb{E}_{q_σ(\tilde{x}|x)}\mathbb{E}_{q_{data}(x)}
\Big[\Big\Vert s_{\theta}( \tilde{x},\sigma ) + \frac{\tilde{x}-x}{σ^2}\Big\Vert_2^2\Big] \\ 
&= \frac{1}{2}\mathbb{E}_{q_{data}(x)}\mathbb{E}_{\tilde{x} \sim \mathcal{N}(\tilde{x}|x,σ^2I)}
\Big[\Big\Vert s_{\theta}( \tilde{x},\sigma ) + \frac{\tilde{x}-x}{σ^2}\Big\Vert_2^2\Big] \tag{2.14} \\
&= \frac{1}{2}\mathbb{E}_{q_{data}(x)}\mathbb{E}_{\tilde{x} \sim \mathcal{N}(\tilde{x}|x,σ^2I)}
\Big[\Big\Vert s_{\theta}( \tilde{x},\sigma ) + \frac{\epsilon}{σ}\Big\Vert_2^2\Big]  \\
& =\mathcal{l}(\theta;\sigma) \tag{2.15}
\end{align}
$$
我们把最后公式（2.14）的写成公式（2.15）方便后面使用。在公式（2.14）中，我们发现，主要的问题在于噪声力度 $\sigma$ 。噪声加的大（极端情况变成纯噪声）会导致加噪之后的数据分布严重偏离原始数据分布；反之，噪声加的小（极端情况变成0噪声）不能很好的解决前述存在的问题。为了权衡这两个方面，作者提出同时使用多个不同大小噪声力度的退火方案。

***
尝试思考：在前文讲VDM时，是方差位未知，这里还是方差位参数位置。他们是不是有什么联系呢？
***
### 8.3.2 退火朗之万动力学采样（annealed Langevin dynamics）

NCSN 使用了多个噪声级别，于是，分别对它们的损失加权求和后再求均值，就得到了联合的损失函数，它表示为：
有 $L$ 个递增的 $0<𝜎_1<𝜎_2<⋯<𝜎_L$ ，分布 $q(X)$ 与每个高斯噪声 $\sigma\epsilon \sim \mathcal{N}(0,𝜎_i^2I),i=1,2,⋯,L$ 结合成受扰动的分布。这里的 $𝜎_1$ 要足够小，$𝜎_𝐿$ 的尺度大概为所有训练数据点两两之间的最大距离，𝐿 一般是几百到几千的范围。这种采样方法越往后，噪声越小（退火），所以叫退火朗之万动力学采样（annealed Langevin dynamics）。直观来看，按照噪声递减的顺序来采样是因为，一开始噪声先大一些，能够让数据先移动到高密度区域，之后噪声再小一些，可以让数据在高密度区域能更精准地移动到更加符合数据分布的位置。
如下是退火朗之万动力学采样（annealed Langevin dynamics）的算法流程：

| 退火朗之万动力学采样伪代码                                                                                                  |
| -------------------------------------------------------------------------------------------------------------- |
| 已知：$\{\sigma_i\}_{i=1}^L,\epsilon,T$                                                                           |
| 1:  初始化 $x_0$ 为随机分布                                                                                            |
| 2:  for i=L to 1:        # 模拟退火算法，噪声从大到小                                                                       |
| 3:        $\alpha_i = \epsilon \cdot \frac{\sigma_i^2}{\sigma_L^2}$     # $\alpha$ 是步长，从大到小                    |
| 4:        for t =1 to T:  # 下面是T步郎之万采样                                                                         |
| 5:             Draw $z_t \sim \mathcal{N}(0,I)$                                                                |
| 6:             $x_{t}=x_{t-1}+\frac{\alpha_i}{2}s_{\theta}(x_{t-1},\sigma_i) +\sqrt{\alpha_i}\boldsymbol{z}_t$ |
| 7:        $x_0=x_t$                                                                                            |
| 8.  return $x_T$                                                                                               |
我们




[1] Jonathan Ho, Ajay Jain, Pieter Abbeel. Denoising Diffusion Probabilistic Models. arXiv preprint  arXiv:2006.11239v2.
[2] Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, Ben Poole. Score-Based Generative Modeling through Stochastic Differential Equations. arXiv:2011.13456
[3] Yang Song, and Stefano Ermon. Generative Modeling by Estimating Gradients of the Data Distribution. arXiv:1907.05600
[4] Pascal Vincent. A Connection Between Score Matching and Denoising Autoencoders