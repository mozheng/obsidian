在生成扩散的蓬勃发展下，**流匹配**（Flow Matching，FM）生成模型凭借独特优势崭露头角。它基于**连续标准化流**（Continuous Normalizing Flows，CNFs），为生成模型训练带来新突破，能让模型学习复杂数据分布并生成高质量样本。接下来，让我们深入探索流匹配生成模型的奥秘。
连续归一化流由一连串的可逆变换构建。连续归一化流模型通过可逆变换将简单的先验分布（如正态噪声）映射到复杂的数据分布，并且可以精确计算数据的概率密度函数。在理论上具有严格的数学基础和良好的性质。通俗的解释就是“流模型”是一个高中就常见的可逆函数。如果 $z$ 是噪声，$x$ 是图片，加噪函数是 $z=f(x)$ ， 去噪采样函数就是 $x=f^{-1}(z)$ 。我们求出 $f^{-1}$ 就可以生成图像了。
在 Stable Diffusion 3 , FLUX两个图片生成范式中，作者采用了新型扩散transformer并结合了流匹配，优化了模型对图像生成过程中的数据流动和信息传递的处理，使生成的图像更加逼真、准确，且能够更好地理解和处理复杂的提示信息。
本文主要内容来自 Yaron Lipman 等人的论文《FLOW MATCHING FOR GENERATIVE MODELING》，后面检测为“流匹配论文”。
### 2.4.1 流匹配基础公式
#### 2.4.1.1 变量替换方程
上面的 $f$ 该怎么求？如果容易求，图像生成恐怕早就进入Flux时代了。问题在于这个函数不好求。别急，我们先介绍“**变量替换方程**”。流模型通过一系列由变量替换方程支撑的可逆变换，来描述这个函数 $f$ 。变量替换方程主要基于雅可比变换（也叫变量替换公式）以及概率密度函数的变换规则推导得出。 
> [!雅可比矩阵]
> 雅可比矩阵是函数对**向量导数**的扩展。如果 $\mathbf{z}=f(\mathbf{x})$ ，雅可比矩阵 $J_f$ 定义为
>$$J_f=\frac{\partial \mathbf{z}}{\partial \mathbf{x}}=
\begin{bmatrix}\frac{\partial z_1}{\partial x_1} & \frac{\partial z_1}{\partial x_2}&\cdots&\frac{\partial z_1}{\partial x_d} \\
\frac{\partial z_2}{\partial x_1} & \frac{\partial z_2}{\partial x_2}&\cdots&\frac{\partial z_2}{\partial x_d} \\
\vdots&\vdots&\ddots&\vdots \\
\frac{\partial z_d}{\partial x_1} & \frac{\partial z_d}{\partial x_2}&\cdots&\frac{\partial z_d}{\partial x_d}
\end{bmatrix}$$
> 雅可比矩阵是一个在数学、物理、工程等多个领域都具有重要意义的概念，在多元积分中，雅可比矩阵的行列式 $|J_f(\mathbf{x})|$ 起到了关键作用。它描述了从原变量 $\mathbf{x}$ 到新变量 $\mathbf{z}$ 的变换过程中，体积微元的变化比例。具体来说，积分 
> $$\int_D g(\mathbf{z})\mathrm{d}\mathbf{z}$$ 
> 在  $\mathbf{z}=f(\mathbf{x})$ 变量替换后，可转化为 
> $$\int_{D'} g(f(\mathbf{x})) |J_f(\mathbf{x})|\mathrm{d}\mathbf{x}$$ 
> 其中 $D$ 和 $D'$ 分别是原变量和新变量下的积分区域。这种积分变换计算，例如从直角坐标到极坐标、柱坐标或球坐标的转换中经常用到。同样的，我们在大学高等数学课程中，也有如下针对于一元函数的换元转换。
> $$\int_D g(z)\mathrm{d}x =\int_{D'} g(f(x))\mathrm{d}f(x)=\int_{D'} g(f(x)) f'(x)\mathrm{d}x$$

我们先给出两个变量替换说明
- **变量设定说明**：$\mathbf{x}$ 是一个 $d$ 维随机变量，其概率密度函数为 $p(\mathbf{x})$。我们通过一系列可逆变换 $f$ 将 $\mathbf{x}$ 变换为另一个 $d$ 维随机变量 $\mathbf{z}$ ，即 $\mathbf{z}=f(\mathbf{x})$，并且 $f$ 是一个一一映射（可逆）且可微的函数。
- **一一映射概率等价说明**：对于一个微小区域 $\mathrm{d}\mathbf{x}$ ，$\mathbf{x}$ 落在这个区域的概率为 $p_\mathbf{x}(\mathbf{x})\mathrm{d}\mathbf{x}$（这里 $\mathrm{d}\mathbf{x}=\mathrm{d}x_1\mathrm{d}x_2\cdot\cdot\cdot\mathrm{d}x_d$）；经过变换后，$z$ 落在对应区域 $\mathrm{d}z$ 的概率为 $p_\mathbf{z}(\mathbf{z})\mathrm{d}\mathbf{z}$ （同样 $\mathrm{d}\mathbf{z}=\mathrm{d}z_1\mathrm{d}z_2\cdot\cdot\cdot\mathrm{d}z_d$）。由于变换是一一映射的，这两个概率应该相等，即 $p_\mathbf{x}(\mathbf{x})\mathrm{d}\mathbf{x}=p_\mathbf{z}(\mathbf{z})\mathrm{d}\mathbf{z}$。
因此我们把 $\mathrm{d}\mathbf{z}=|\det(J_f(\mathbf{x}))|\mathrm{d}\mathbf{x}$ 代入 $p_\mathbf{x}(\mathbf{x})\mathrm{d}\mathbf{x}=p_\mathbf{z}(\mathbf{z})\mathrm{d}\mathbf{z}$ 可得：
$$\begin{align}
p_\mathbf{x}(\mathbf{x})\mathrm{d}\mathbf{x} &=p_\mathbf{z}(\mathbf{z})\mathrm{d}\mathbf{z} \tag{}\\
&= p_\mathbf{z}(\mathbf{z})|J_f(\mathbf{x})|\mathrm{d}\mathbf{x}  \tag{}\\
\rightarrow p_\mathbf{x}(\mathbf{x}) &=p_\mathbf{z}(f(\mathbf{x}))|J_f(\mathbf{x})| \tag{2.4.1}
\end{align}$$
我们在最后一步消掉了两边的 $\mathrm{d}\mathbf{x}$ 得到公式（2.4.1）。这就是我们在流模型中核心使用的**变量替换方程**。
> [!warning]
>  在流匹配论文中，使用 $x=f(z)$ 从噪声到图像的生成逻辑推理公式，那么有
>  $$p_\mathbf{x}(\mathbf{x}) =p_\mathbf{z}(f^{-1}(\mathbf{x}))|J_{f^{-1}}(\mathbf{x})|$$ 
>  再根据反函数定理 $|J_{f^{-1}}(\mathbf{x})|=|J_{f}(\mathbf{x})|^{-1}$  可得
>   $$p_\mathbf{x}(\mathbf{x}) =p_\mathbf{z}(f^{-1}(\mathbf{x}))|J_{f}(\mathbf{x})|^{-1}=p_\mathbf{z}(f^{-1}(\mathbf{x}))\det[\frac{\partial \mathbf{f}^{-1}}{\partial \mathbf{x}}(x)] \tag{2.4.2}$$ 
>   如果在其他地方看到这两种形式，请不要疑虑，本质相同。

#### 2.4.1.2 连续标准化流
**连续标准化流** （Continuous Normalizing Flows, CNFs）表述如下：在 $d$ 维数据空间$\mathbb{R}^{d}$中，有概率密度路径 $p : [0, 1]×\mathbb{R}^{d}→\mathbb{R}_{>0}$ 和时间依赖向量场 $v : [0, 1]×\mathbb{R}^{d}→\mathbb{R}^{d}$ 。根据向量场 $v_{t}$ 可构建流 $\phi : [0, 1]×\mathbb{R}^{d}→\mathbb{R}^{d}$ ，两者关系为 $\frac{d}{dt}\phi_{t}(x)=v_{t}(\phi_{t}(x)),\phi_0(x)=x$ 。这里的向量场 $v_{t}$ 用神经网络$v_{t}(x;\theta)$ 建模。这里 $\phi_t$ 是CNF 。
通俗地讲的就是：现有一个取值范围编排0到1的时间序列 $t$，不同于之前扩散模型的时间 $t$ 为整数，这里的 $t$ 标准化到 $[0,1]$ 区间，本质上是为了更好地参与积分计算。因为有 $t$ ，所以有 $t$ 时间点的概率密度路径 $p_t$ ， $t$ 时间点的 $\phi_t$ （对应上文中的 $f^{-1}$），以及 $t$ 时间点的时间依赖向量场 $v_t$ 。我们常常使用参数 $\theta$ 代表需要建模的模型，又因为 $v_t,\phi_t$ 同源， 所以这里用 $v_{t}(x;\theta)$ 建模来表示 $\phi_t$ 。这里 $\phi_t$ 是CNF，连续的CNF集合形成CNFs。
我们可以将上面的向量场 $v_t$ 理解为 $\phi_t$ 的导数。但其实设计这个参数符号的人是想让你往“**速度**”的角度进行理解。我们可将简单噪声 $x_0$ 到生成图像的复杂分布 $x_1$ 看成一条t路径，这里称为“**条件概率路径**”。向量场 $v_t$ 就是在这条路径上的交通工具行驶的速度。那么连续标准化流的采样方法变成了小学数学中我们常用的“**速度-距离**”方程： $x_{t+\Delta t} = x_t + \Delta t v_t$ 。

>[!注意]
>与之前的扩散模型不一样，这里的开始时间 $t=0$ 是简单噪声，归一化后的结束时间 $t=1$ 为生成图像后的时间。同时，为了让适配更通用说法，我们不将生成目标假定为图片，而是将 $t=0$ 的 $p_0$ 称为**简单正态分布**，你可以认为是  $p_0(x)=\mathcal{N}(0,\mathbf{I})$; $t=1$ 的 $p_1$ 称为**复杂的生成正态分布**。


	这里之所以用两种方法阐述，当然是为了即照顾到数学的严谨性，又照顾到读者的理解。所以大家以后在看数学公式时，试着自己用土话或大白话翻译一下一大坨公式究竟要干什么。

根据流模型变量替换公式（2.4.2）我们可得 0-1时间段的某个时间点 $t$ 的概率密度路径 $p_t$ 可表示为
$$p_{t}=p_{0}(\phi_t^{-1})\det[\frac{\partial \mathbf{\phi_t^{-1}}}{\partial \mathbf{x}}(x)]$$
 $p_0$ 表示一种简单的初始分布，通常是标准正态分布。这里使用 $p$ 作为概率函数表示，其命名逻辑与上一章的逻辑一样，依然代表“从原始噪声数据推断到生成图片数据”的过程。流匹配论文中使用下角标 * 运算符，将公式简写为
$$p_{t}=p_{0}(\phi_t^{-1})\det[\frac{\partial \mathbf{\phi_t^{-1}}}{\partial \mathbf{x}}(x)]=[\phi_t]_* p_0 \tag{2.4.3}$$
公式（2.4.3）就是CNF的**前推公式**（push-forward equation）。请注意，这里与DDPM的前向加噪公式不同。
我们盘点一下我们现有的资料。假设训练数据样本 $q(x_1)$ ，但无法访问密度函数本身。这里 $p_1,q$ 本质一致，都是最终生成的复杂正态分布，这里用另一个符号的内在含义就是与上文中的 $p$ 区分开。
上文中模型训练的核心目标是使 $p_1$ 的分布与 $q$ 尽可能相似。通过构建合适的概率路径 $p_t$ ，让 $p_1$ 不断逼近 $q$ 。具体怎样构建这样的合适概率路径，这就是**流匹配**（FM，CFM）的问题了。
### 2.4.2 流匹配（FM）原理 
#### 2.4.2.1 流匹配目标函数
虽然我们已经有个（2.4.3）公式了，但是其中的 $\phi_t$ （或者与其相关的 $v_t$ ）怎么求我们并不知道。
我们换种思维，选择基于速度 $v_t$ 建模。假设目标概率密度是 $p_{t}(x)$ ，其对应的向量场为 $u_{t}(x)$（向量场 $u_{t}(x)$ 可看做上面以 $\theta$ 为参数的速度模型 $v_t(x,\theta)$ 的groundtruth）。那么我们可以，通过这样的**流匹配（FM）目标函数**来训练模型 $v_t(x,\theta)$：
$$\mathcal{L}_{FM}(\theta)=\mathbb{E}_{t,p_{t}(x)}\left\| v_{t}(x,\theta)-u_{t}(x)\right\| ^{2}$$
其中 $\theta$ 是CNF向量场 $v_{t}$ 的可学习参数， $t \sim U[0, 1]$，$x \sim p_{t}(x)$  。直观理解，FM就是用神经网络 $v_{t}$ 去逼近向量场 $u_{t}$ ，当损失为0时，学习到的CNF模型就能生成 $p_{t}(x)$ 。理想很丰满，现实很闹心。向量场为 $u_{t}(x)$ 是什么我们并不知道。基于DDPM的经验，如果当前概率不知道，你加个条件不就完了吗？DDPM里面不知道 $q(x_{t−1}|x_t)$ ，所以往条件里面加个一个原始图像 $x_0$ ，形成求 $q(x_{t−1}|x_t,x_0)$ 的局面。流模型中我们想生成的复杂分布对象是 $x_1$ 。顺着DDPM的推导思路。我们自然想到可以求条件向量场 $u_{t}(x|x_{1})$ ，同配套的还有 $p_{t}(x|x_{1})$ 。因此接下来做的就是要探讨“**条件流匹配**（CFM）与当前**流匹配**（FM）有何关系”，“CFM该如何优化求解”的问题了，那是2.4.3节的内容了。
但本节还没有这么快结束，请允许我插入一条支线“**连续性方程**”。没有这个条件，后面就无法推理。

 >[!连续性方程]
 >连续性方程是一个物理概念，如 $、rho_t(x)$ 满足连续性方程，则有：
$$\frac{\mathrm{d}}{\mathrm{d}t}\rho_t(x)= -div(u_t(x)\rho_t(x))$$
>我们粗浅地理解下这个公式。在流体物理学中，流入=流出，即可得 “**通过一个截面的流体质量变化量=通过这个截面的流体的流量 * 这个截面的密度**” 。因为散度是个向量，方向相反，所以前面有负号。有人说这种相等不是很正常的事吗？并不是，流淌中断，没有算完全部支流等不可避免的原因都不会使等式相等。我们这里是用计算机模拟自然，是可以应用到理论的结果。
>
>在基于 CNFs 的生成模型中，满足连续性方程是确保模型合理性和有效性的关键。它保证了所构建的概率路径 $p_t(x)$ 能够通过合适的向量场 $u_t(x)$ 进行合理的变换，从而实现从简单先验分布到复杂数据分布的映射。如果不满足连续性方程，那么概率分布的变换将不符合物理和概率的基本原理，模型就无法准确地对数据进行建模。

**在这里，$u_t$ 和 $p_t$ 同时满足连续性方程，我们证明如下。** 
根据条件全概率公式, 使用 $x_1$ 划分方程可得：
$$p_t(x)=\int p_t(x|x_1)q(x_1)\mathrm{d}x_1$$
如果 $p_t(x)$ 对 $t$ 求微分，还由于上文中只有 $p_t(x|x_1)$ 有 $t$，所以有
$$\frac{\mathrm{d}}{\mathrm{d}t}p_t(x)=\int (\frac{\mathrm{d}}{\mathrm{d}t}p_t(x|x_1))q(x_1)\mathrm{d}x_1$$


### 2.4.3 条件流匹配（CFM）优化 

通过构建条件概率路径 $p_{t}(x|x_{1})$ 和条件向量场 $u_{t}(x|x_{1})$ ，对其进行“边缘化”得到边际概率路径 $p_{t}(x)$ 和边际向量场 $u_{t}(x)$ 。 $q$ 函数代表未知的数据分布，是模型想要逼近的真实分布，模型训练的目的就是让尽可能接近CFM目标函数为
$$\mathcal{L}_{CFM}(\theta)=\mathbb{E}_{t,q(x_{1}),p_{t}(x|x_{1})}\left\| v_{t}(x,\theta)-u_{t}(x|x_{1})\right\| ^{2}$$
令人惊喜的是，FM和CFM目标函数对$\theta$的梯度相同，优化CFM目标函数等同于优化FM目标函数，这让训练CNF模型变得可行。
### 2.4.4 条件概率路径与向量场构建 

三定理

**
### （一）高斯条件概率路径通用形式 
考虑高斯条件概率路径$p_{t}(x|x_{1})=\mathcal{N}(x|\mu_{t}(x_{1}),\sigma_{t}(x_{1})^{2}I)$，其中$\mu_{t}(x_{1})$是时变均值，$\sigma_{t}(x_{1})$是时变标准差。通常设定$\mu_{0}(x_{1}) = 0$，$\sigma_{0}(x_{1}) = 1$，使$t = 0$时所有条件概率路径收敛到标准高斯噪声分布；$\mu_{1}(x_{1}) = x_{1}$，$\sigma_{1}(x_{1})=\sigma_{min}$（足够小），让$p_{1}(x|x_{1})$是集中在$x_{1}$的高斯分布。对应的向量场$u_{t}(x|x_{1})=\frac{\sigma_{t}'(x_{1})}{\sigma_{t}(x_{1})}(x-\mu_{t}(x_{1}))+\mu_{t}'(x_{1})$，能生成该高斯路径。 
### （二）特殊实例：扩散与最优传输 1. **扩散条件向量场**：扩散模型的概率路径可看作特殊的高斯条件概率路径。如反向（噪声→数据）方差爆炸（VE）路径$p_{t}(x)=\mathcal{N}(x|x_{1},\sigma_{1 - t}^{2}I)$，反向方差保持（VP）扩散路径$p_{t}(x|x_{1})=\mathcal{N}(x|\alpha_{1 - t}x_{1},(1-\alpha_{1 - t}^{2})I)$ 。用本文方法构建的条件向量场与之前确定性概率流中的向量场一致，但结合FM目标函数训练，比传统分数匹配方法更稳定、鲁棒。 2. **最优传输（OT）条件向量场**：定义$\mu_{t}(x)=tx_{1}$，$\sigma_{t}(x)=1-(1-\sigma_{min})t$ ，得到的条件向量场$u_{t}(x|x_{1})=\frac{x_{1}-(1-\sigma_{min})x}{1-(1-\sigma_{min})t}$ ，对应的条件流$\psi_{t}(x)=(1-(1-\sigma_{min})t)x+tx_{1}$是两个高斯分布间的最优传输位移映射。与扩散路径相比，OT路径粒子运动轨迹是直线，方向恒定，更易于用参数模型拟合，训练和采样效率更高。 ## 四、模型训练与实验验证 ### （一）实验设置 在CIFAR10和不同分辨率的ImageNet数据集上展开实验。模型架构采用U - Net，对比FM与其他基于扩散的方法，如DDPM、分数匹配（Score Matching）、Score Flow等。训练时使用相同架构、超参数和训练轮数，部分基线方法为更好收敛允许更多迭代。 ### （二）实验结果分析 1. **密度建模与样本质量**：在CIFAR10和ImageNet数据集上，使用OT路径的FM（FM - OT）在负对数似然（NLL）、弗雷歇 inception距离（FID）等指标上表现最优。例如在ImageNet 128×128分辨率下，FM - OT的FID达到20.9，优于诸多先进生成对抗网络（GAN）模型，证明其在密度建模和生成高质量样本方面的优势。 2. **采样效率**：从采样路径看，OT路径模型比扩散路径模型更早生成图像。在低计算成本采样（固定步长求解器，低NFE）实验中，FM - OT模型在数值误差和样本质量上表现出色，达到相同误差阈值所需NFE约为扩散模型的60%，在样本质量和计算成本间取得更好平衡。 3. **条件采样**：在条件图像生成（如将64×64图像上采样到256×256）实验中，FM - OT在FID和IS指标上表现优异，相比基线方法有显著提升，表明其在条件生成任务中的有效性。 ## 五、流匹配生成模型优势与应用前景 ### （一）优势总结 1. **训练高效稳定**：FM摆脱传统扩散模型限制，通过直接指定概率路径训练，收敛速度快。在ImageNet 64×64训练中，FM - OT降低FID的速度更快、幅度更大，且采样成本在训练中保持稳定。 2. **采样灵活快速**：基于OT路径的FM采样效率高，粒子运动轨迹简单，能快速生成高质量样本。在不同ODE求解器下，FM - OT模型采样效率都最高，减少计算资源消耗。 3. **可扩展性强**：CFM基于样本构建概率路径和向量场，使FM能轻松扩展到高维数据，适用于复杂图像生成任务。 ### （二）应用前景展望 1. **图像生成领域**：可用于高质量图像生成，如艺术创作、虚拟场景构建等。生成的图像在视觉效果和多样性上有望进一步提升，满足不同应用场景需求。 2. **其他领域拓展**：在语音合成、数据增广、异常检测等领域，流匹配生成模型也可能发挥重要作用。通过学习复杂数据分布，为这些领域的研究和应用提供新方法。